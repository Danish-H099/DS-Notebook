{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92664c2",
   "metadata": {},
   "source": [
    "# scikit-learn \n",
    "- **scikit-learn**, often abbreviated as <mark>sklearn</mark>,is an `open-source Python library` that implements a range of \n",
    " - machine learning\n",
    " - pre-processing\n",
    " - cross-validation\n",
    " - visualization algorithms<br>using a unified interface.\n",
    "\n",
    "\n",
    "- Simple and efficient tools for <mark>data mining</mark> and <mark>data analysis</mark>. \n",
    "\n",
    "\n",
    "- It includes implementations of various **supervised and unsupervised learning algorithms** such as:\n",
    " - support vector machines (SVM)\n",
    " - random forests\n",
    " - k-nearest neighbors (KNN)\n",
    " - k-means clustering...\n",
    "\n",
    "\n",
    "- It provides a wide range of tools for various <mark>**ML tasks**</mark> such as:\n",
    "  - classification\n",
    "  - Regression\n",
    "  - Clustering\n",
    "  - Dimensionality reduction\n",
    "  - Model selection.\n",
    "\n",
    "\n",
    "- Built on the top of NumPy, SciPy, and matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab9fe7",
   "metadata": {},
   "source": [
    "## How to implement sklearn on a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c8c28",
   "metadata": {},
   "source": [
    "### Step1 : Import the relevant modules: \n",
    "Start by importing the necessary modules from scikit-learn as well as other Python libraries for data manipulation and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61d5c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6f693",
   "metadata": {},
   "source": [
    "- imports the **`load_iris function`** from the <mark>datasets **module**</mark> in **scikit-learn**. \n",
    "<br>\n",
    "- This function is used to load the **Iris dataset**, which is a **classic dataset** in machine learning and statistics. It is often used for **learning and testing purposes**.\n",
    "<br>\n",
    "- The Iris dataset consists of 150 samples of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f523f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e1097",
   "metadata": {},
   "source": [
    "- imports the **`train_test_split function`** from the <mark>model_selection **module**</mark> in **scikit-learn**. \n",
    "<br>\n",
    "- This function is commonly used for <mark>**splitting datasets** into training and testing subsets.</mark>\n",
    "<br>\n",
    "- By default, it splits the data into **`75% training and 25% testing sets`**, but you can adjust this ratio with the **test_size parameter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f48304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549061b9",
   "metadata": {},
   "source": [
    "- imports the **`StandardScaler class`** from the <mark>preprocessing **module**</mark> in scikit-learn. \n",
    "<br>\n",
    "- The StandardScaler class is used for <mark>**standardizing features**</mark> by removing the mean and scaling to unit variance.\n",
    "<br>\n",
    "- Standardization is a common preprocessing step in machine learning where the features are transformed in such a way that they have **`mean 0 and variance 1.`**\n",
    "<br>\n",
    "- This preprocessing step is important for algorithms that are sensitive to the scale of the features, such as\n",
    " - support vector machines (SVM)\n",
    " - k-nearest neighbors (KNN)\n",
    " - logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a862877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda5002",
   "metadata": {},
   "source": [
    "- imports the **`LogisticRegression class`** from the <mark>linear_model **module**</mark> in scikit-learn. \n",
    "<br>\n",
    "- The LogisticRegression class is used for <mark>**logistic regression**</mark>, which is a statistical method used for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9384425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c05fca",
   "metadata": {},
   "source": [
    "- imports the **`accuracy_score function`** from the <mark>metrics **module**</mark> in scikit-learn. \n",
    "<br>\n",
    "- The accuracy_score function is used to <mark>**evaluate the accuracy**</mark> of a classification model's predictions compared to the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b92a0",
   "metadata": {},
   "source": [
    "### Step 2 : Prepare your data: \n",
    "Load your dataset and preprocess it as necessary. This might involve tasks such as \n",
    "- handling missing values\n",
    "- encoding categorical variables\n",
    "- splitting the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f5b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data # Features (sepal length, sepal width, petal length, petal width)\n",
    "y = iris.target # Target (species labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0794817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266a72b",
   "metadata": {},
   "source": [
    "- **X_train**: Features for training\n",
    "- **X_test**: Features for testing\n",
    "- **y_train**: Target labels for training\n",
    "- **y_test**: Target labels for testing\n",
    "<br>\n",
    "\n",
    "\n",
    "- **test_size=0.2**: Specifies that 20% of the data will be reserved for testing, and the remaining 80% will be used for training.\n",
    "- **random_state=42**: Sets the random seed to 42, ensuring reproducibility. This means that each time you run this code, the data will be split in the same way, which is useful for debugging and comparing different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7bebb",
   "metadata": {},
   "source": [
    "### Step 3 : Choose a model: \n",
    "Select an appropriate machine learning algorithm for your task based on the type of problem you're trying to solve (classification, regression, clustering, etc.) and the characteristics of your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598e2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Choose a model\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c05095",
   "metadata": {},
   "source": [
    "- Now, you can use this model to fit your training data and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa56dc6",
   "metadata": {},
   "source": [
    "### Step 4 : Train the model:\n",
    "Fit the chosen model to your training data. This involves using the fit() method provided by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7025546e",
   "metadata": {},
   "source": [
    "### Step 5 : Evaluate the model: \n",
    "Assess the performance of your model using appropriate evaluation metrics. For supervised learning tasks, this often involves making predictions on a test set and comparing them to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098ebdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a731f",
   "metadata": {},
   "source": [
    "- **y_pred = model.predict(X_test)**: This line uses the trained LogisticRegression model (model) to predict the labels for the test data (X_test). The predict method takes the features of the test data (X_test) as input and returns the predicted labels (y_pred). These predicted labels will be used to evaluate the performance of the model.\n",
    "\n",
    "- **accuracy = accuracy_score(y_test, y_pred)**: This line calculates the accuracy of the predictions made by the model. The accuracy_score function from scikit-learn compares the true labels of the test data (y_test) with the predicted labels (y_pred) and computes the accuracy as the fraction of correctly classified samples. The result is stored in the variable accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75290a4a",
   "metadata": {},
   "source": [
    "### Step 6 : Tune hyperparameters:\n",
    "Fine-tune the hyperparameters of your model to optimize its performance. This can be done using techniques like grid search or randomized search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836cae73",
   "metadata": {},
   "source": [
    "### Step 7 : Make predictions: \n",
    "Once you're satisfied with your model's performance, you can use it to make predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4475c",
   "metadata": {},
   "source": [
    "# Algorithms available in Sklearn\n",
    "\n",
    "Linear Models:\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- ElasticNet\n",
    "\n",
    "Support Vector Machines (SVM):\n",
    "\n",
    "- Support Vector Classifier (SVC)\n",
    "- Support Vector Regression (SVR)\n",
    "\n",
    "Tree-based Methods:\n",
    "\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Gradient Boosting Machines (GBM)\n",
    "- AdaBoost\n",
    "\n",
    "Nearest Neighbors:\n",
    "- k-Nearest Neighbors (KNN)\n",
    "\n",
    "Clustering:\n",
    "- K-Means\n",
    "- Agglomerative Hierarchical Clustering\n",
    "- DBSCAN\n",
    "\n",
    "Dimensionality Reduction:\n",
    "- Principal Component Analysis (PCA)\n",
    "- Singular Value Decomposition (SVD)\n",
    "- t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Naive Bayes:\n",
    "- Gaussian Naive Bayes\n",
    "- Multinomial Naive Bayes\n",
    "- Bernoulli Naive Bayes\n",
    "\n",
    "Ensemble Methods:\n",
    "- Voting Classifier\n",
    "- Voting Regressor\n",
    "- Bagging\n",
    "- Stacking\n",
    "\n",
    "Neural Networks (via integration with libraries like TensorFlow or PyTorch):\n",
    "- Multi-layer Perceptron (MLP)\n",
    "\n",
    "Supporting algorithms:\n",
    "- Gradient Descent Optimizers\n",
    "- Loss functions\n",
    "- Various metrics for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be434f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
